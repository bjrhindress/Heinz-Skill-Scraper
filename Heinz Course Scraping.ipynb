{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen  # b_soup_1.py\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must run the following commands in the appropriate conda env in order for this to work:\n",
    ">  \\> conda install spacy  \n",
    ">  \\> python -m spacy download en   \n",
    " (second one must be run as admin)\n",
    "\n",
    "Alternatively run the top command and then replace the cell below with:  \n",
    "`import spacy  \n",
    "import en_core_web_sm  \n",
    "nlp = en_core_web_sm.load()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver\n",
    "def get_skill_map(skills_list):\n",
    "    \"\"\"\n",
    "    Returns a dict mapping each skill in the skill_list\n",
    "    to a list of course names associated with that skill.\n",
    "    \"\"\"\n",
    "    # first get the links for all Heinz courses\n",
    "    course_dict = get_course_links()\n",
    "    course_descriptions = course_dict.copy()\n",
    "\n",
    "    # then scrape the description text for each link\n",
    "    for name, link in course_dict.items():\n",
    "        course_descriptions[name] = get_course_description(link)\n",
    "        \n",
    "    # time for a little nlp (to lemmatize the descriptions)\n",
    "    # we'll store the results in a new dictionary\n",
    "    parsed_descriptions = course_descriptions.copy()\n",
    "    for name, text in course_descriptions.items():\n",
    "        parsed_text = nlp(text)\n",
    "        parsed_tokens = []\n",
    "        \n",
    "        # remove all stopwords, punctuation, and spaces\n",
    "        for token in parsed_text:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if not (nlp.vocab[lemma].is_stop or token.pos_ == 'PUNCT' or token.pos_ == 'SPACE'):\n",
    "                parsed_tokens.append(lemma)\n",
    "        \n",
    "        # finally add list to dictionary\n",
    "        parsed_descriptions[name] = parsed_tokens\n",
    "    \n",
    "    \n",
    "    # now we can iterate through the skills list and build our final dictionary\n",
    "    # give each skill its own list\n",
    "    skills_to_courses = { k : [] for k in skill_list }\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        # now check every course for that skill\n",
    "        for course_name in parsed_descriptions.keys():\n",
    "            # if that skill appears in the description, add to the map\n",
    "            if skill.lower() in parsed_descriptions[course_name]:\n",
    "                skills_to_courses[skill].append(course_name)\n",
    "    \n",
    "    # return the final mapping\n",
    "    return skills_to_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_links():\n",
    "    \"\"\"\n",
    "    Get a list of urls to Heinz College course description \n",
    "    pages, so we can scrape each of those iteratively.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    skill_list\n",
    "        A list of all the skills searched for in these pages\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    <dict> {course_name: course_link, ...}\n",
    "        course_name: string containing course name\n",
    "        course_link: string containing link to course description page\n",
    "    \"\"\"\n",
    "    html_base = 'https://api.heinz.cmu.edu'\n",
    "    html = urlopen('https://api.heinz.cmu.edu/courses_api/course_list/')\n",
    "    soup = BeautifulSoup(html.read(), \"lxml\")\n",
    "    \n",
    "    names = []\n",
    "    links = []\n",
    "    \n",
    "    # Each course link is contained in a <tr> element with class=\"clickable-row\"\n",
    "    course_rows = soup.findAll('tr', {'class': 'clickable-row'})\n",
    "    for row in course_rows:\n",
    "        names.append(row.find_all('td')[1].string)\n",
    "        links.append(html_base + row.get('data-href'))\n",
    "    \n",
    "    return dict(zip(names, links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_description(link):\n",
    "    # Retrieve the html\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html.read(), \"lxml\")\n",
    "\n",
    "    # Recover the description text\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    clean = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://api.heinz.cmu.edu/courses_api/course_list/')\n",
    "soup = BeautifulSoup(html.read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_rows = bsyc.findAll('tr', {'class': 'clickable-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.heinz.cmu.edu/courses_api/course_detail/90-707'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_links = get_course_links()\n",
    "course_links['Statistical Reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(course_links['Statistical Reasoning'])\n",
    "soup = BeautifulSoup(html.read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statistical Reasoning Units 12 Description This course will provide an introduction to the principles of data collection description and analysis You will learn the basic tools of statistical inference and modeling as well as how to interpret results measure the uncertainty in result and understand the limitations of results You will also learn how to interpret statistical output and how not to be fooled by statistical studies Learning Outcomes At the end of this course you should be able to\\r\\n Present data visually in tabular and graphic form\\r\\n Summarize a set of observations by reporting a measure of center and dispersion\\r\\n Explain how and why sample data can be used to estimate descriptive measures of populations when census data is unavailable and how we measure the accuracy and precision of the estimate\\r\\n Apply the basic rules of probability\\r\\n Find and interpret the probability for a random variable which has a normal distribution\\r\\n Explain how to take a proper scientific sample that can be used to make inferences about the larger population\\r\\n Explain what sampling error is and why it exists\\r\\n Classify data by type and use the proper summary statistics and tests for the data type\\r\\n Interpret the pvalue test statistic and other Minitab output from a test of hypotheses confidence interval and linear regression Syllabus AU_90707__Statistical_Reasoning_Syllabus_S19pdf'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = get_course_description(course_links['Statistical Reasoning'])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statistical Reasoning Units: 12 Description: This course will provide an introduction to the principles of data collection, description and analysis. You will learn the basic tools of statistical inference and modeling, as well as how to interpret results, measure the uncertainty in result, and understand the limitations of results. You will also learn how to interpret statistical output, and how not to be fooled by statistical studies. Learning Outcomes: At the end of this course, you should be able to\\r\\n- Present data visually in tabular and graphic form\\r\\n- Summarize a set of observations by reporting a measure of center and dispersion\\r\\n- Explain how and why sample data can be used to estimate descriptive measures of populations when census data is unavailable, and how we measure the accuracy and precision of the estimate\\r\\n- Apply the basic rules of probability\\r\\n- Find and interpret the probability for a random variable which has a normal distribution\\r\\n- Explain how to take a proper scientific sample that can be used to make inferences about the larger population\\r\\n- Explain what sampling error is and why it exists\\r\\n- Classify data by type and use the proper summary statistics and tests for the data type\\r\\n- Interpret the p-value, test statistic and other Minitab output from a test of hypotheses, confidence interval, and linear regression Syllabus: AU_90-707__Statistical_Reasoning_Syllabus_S19.pdf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "test_list = ['Management', 'software', 'knowledge']\n",
    "results = get_skill_map(test_list)\n",
    "write_to_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
